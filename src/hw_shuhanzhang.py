# -*- coding: utf-8 -*-
"""HW#4_ShuhanZhang.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VMRYout4Etgl2uW4-8q3-exj6u4oOrUi

##Part A
"""

#load libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
import numpy as np

"""1. Import the data. The headlines will become your vectorized X matrix, and the labels indicate a binary classification (clickbait or not)."""

#Import the datasets
text_data=pd.read_csv("/text_training_data.csv")

print(text_data.head())

"""2. Convert the headline data into an X feature matrix using a simple bag of words approach."""

from sklearn.feature_extraction.text import CountVectorizer

X_text = text_data["headline"]
y = text_data["label"]

#vectorization
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(text_data['headline'])

print("Shape of X matrix:", X.shape)

"""3. Run logistic regression to predict clickbait headlines. Remember to train_test_split your data and use GridSearchCV to find the best value of C. You should evaluate your data with F1 scoring."""

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import make_scorer, f1_score, classification_report

#Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


#logistic regression model with simple bad of words approach
logreg = LogisticRegression(max_iter=2000)
param_grid = {"C": np.logspace(-3, 3, 10)}

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
f1_clickbait = make_scorer(f1_score, pos_label="clickbait")
grid = GridSearchCV(estimator=logreg, param_grid=param_grid, scoring=f1_clickbait, cv=cv, n_jobs=-1,return_train_score=True)
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best mean CV F1:", grid.best_score_)

y_pred = grid.predict(X_test)

test_f1 = f1_score(y_test, y_pred, pos_label="clickbait")
print("\nTest F1-score:", test_f1)

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

"""4. Run 2 more logistic regression models by changing the vectorization approach (e.g. using n-grams, stop_words, and other techniques we discussed). In both cases, keep your logistic regression step the same. Only change how you're generating the X matrix from the text data."""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

#Model 2: Unigrams+bigrams, stopwords
vectorizer2 = CountVectorizer(stop_words="english", ngram_range=(1, 2))
X2 = vectorizer2.fit_transform(text_data["headline"])
print("X2 shape:", X2.shape)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.2, random_state=42, stratify=y)

logreg = LogisticRegression(max_iter=2000)
param_grid = {"C": np.logspace(-3, 3, 10)}
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
f1_clickbait = make_scorer(f1_score, pos_label="clickbait")

grid2 = GridSearchCV(estimator=logreg, param_grid=param_grid, scoring=f1_clickbait, cv=cv, n_jobs=-1,return_train_score=True)
grid2.fit(X2_train, y2_train)
print("Best C:", grid2.best_params_)
print("Best CV F1:", grid2.best_score_)

y2_pred = grid2.predict(X2_test)
print("\nTest F1:", f1_score(y2_test, y2_pred, pos_label="clickbait"))
print(classification_report(y2_test, y2_pred, digits=4))


# Model 3: Use TF-IDF with character n-grams
vectorizer3 = TfidfVectorizer(analyzer="char", ngram_range=(3, 5),min_df=3)
X3 = vectorizer3.fit_transform(text_data["headline"])
print("X3 shape:", X3.shape)

# Train/test split
X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y, test_size=0.2, random_state=42, stratify=y)
grid3 = GridSearchCV(estimator=logreg,param_grid=param_grid, scoring=f1_clickbait,cv=cv,n_jobs=-1,return_train_score=True)
grid3.fit(X3_train, y3_train)
print("Best C:", grid3.best_params_)
print("Best CV F1:", grid3.best_score_)

y3_pred = grid3.predict(X3_test)
print("\nTest F1:", f1_score(y3_test, y3_pred, pos_label="clickbait"))
print(classification_report(y3_test, y3_pred,digits=4))

"""5. Which of your 3 models performed best? What are the most significant coefficients in each, and how do they compare?"""

def show_coef(grid, vectorizer):
    """
    Prints top 5 positive and top 5 negative coefficients for a trained logistic regression model.
    """
    # Best model
    best_model = grid.best_estimator_
    coefs = best_model.coef_[0]

    # Feature names
    feature_names = vectorizer.get_feature_names_out()

    # Sort by coefficient value
    sorted_idx = np.argsort(coefs)

    # Bottom 5 = most negative (predict NOT clickbait)
    print("\npredicts NOT clickbait 5\n")
    for idx in sorted_idx[:5]:
        print(f"{feature_names[idx]:<25} {coefs[idx]:.5f}")

    # Top 5 = most positive (predict CLICKBAIT)
    print("\npredicts clickbait\n")
    for idx in sorted_idx[-5:]:
        print(f"{feature_names[idx]:<25} +{coefs[idx]:.5f}")

#Model 1
show_coef(grid, vectorizer)

#Model 2
show_coef(grid2, vectorizer2)

#Model 3
show_coef(grid3, vectorizer3)

"""My third model using character Tf-idf with 3 to 5 character n-grams performs the best. In model 1, for predicts not clickbait, "2015" is most siginificant. For predicts clickbait, "pitch" is the the most significant one. In model 2, "know" is most significant for not clickbait and "kills" is most significant for clickbait. In model 3, "'s" is most significant in not clickbait and "s," is most significant in clickbait.

In model 1, which caputures the sigle words, clickbait is associated with emotional outcome words, like "pitch", "lose", "win", etc. The not clickbait is relaed with more factual words. In model 2, the n-grams captures topics and phrases. The clickbait is more related with political and emotional words like "dies" and "obama." The not clickbait is related with more professional tones, such as "guess" and "actually." The model 3 captures more punctuation. It identifies sensational fragments, which may caputure more dramatic pieces.

##Part B

1. Load the data
"""

df=pd.read_csv("http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv")

print(df.head())

"""2. Using the Sequential interface in Keras, build a model with 2 hidden layers with 16 neurons in each. Compile and fit the model. Assess its performance using accuracy on data that has been train_test_split."""

import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import SGD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

model = Sequential()

#Define X and y
feature_cols = ["Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width"]
X = df[feature_cols].values

#Encode species
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df["Species"])

#Tran test split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42,stratify=y)

#Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model.add(Dense(16, activation="relu", input_shape=(X_train.shape[1],)))
model.add(Dense(16, activation="relu"))
model.add(Dense(3, activation="softmax"))

#complie the model
model.compile(optimizer="adam",loss="sparse_categorical_crossentropy", metrics=["accuracy"])

#fit the model
history = model.fit(X_train, y_train, epochs=50, batch_size=16)

#score = model.evaluate(X_test, y_test, batch_size=128)
#print("\n", score)

loss, acc = model.evaluate(X_test, y_test, batch_size=128)
print("Test accuracy:", acc)

"""3. Run 2 additional models using different numbers of hidden layers and/or hidden neurons."""

# Model 2: 7 hidden layers and 50 neurons
model2 = Sequential()
model2.add(Dense(50, activation="relu", input_shape=(X_train.shape[1],)))

# Add 6 more hidden layers of 50 neurons
for _ in range(6):
    model2.add(Dense(50, activation="relu"))

model2.add(Dense(3, activation="softmax"))

model2.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

print(model2.summary())

history2 = model2.fit(X_train, y_train, epochs=50, batch_size=16)

loss2, acc2 = model2.evaluate(X_test, y_test, verbose=0)
print(f"\nModel 2 – Test accuracy: {acc2:.4f}")

#Model 3: 1 hidden layer, 4 neurons
model3 = Sequential()
model3.add(Dense(4, activation="relu", input_shape=(X_train.shape[1],)))
model3.add(Dense(3, activation="softmax"))

model3.compile(optimizer="adam",loss="sparse_categorical_crossentropy",metrics=["accuracy"])

print(model3.summary())

history3 = model3.fit(X_train, y_train,epochs=50,batch_size=16)

loss3, acc3 = model3.evaluate(X_test, y_test, verbose=0)
print(f"\nModel 3 – Test accuracy: {acc3:.4f}")

"""4. How does the performance compare between your 3 models?

My first model with 2 hidden layers with 16 neurons in each performs the best. Model 1 has a strong balance between depth and capacity. It can model the nonlinear boundaries among differnt classes without excessively overfitting. The model 2 is much larger, but the Iris dataset has only 150 observations. So, it doesn't improve generalization, leading to overfit problems. Model 3 is too simple. model can't capture the complexity of the data, reflecting the underfitting since the lack of sufficient complexity to capture the class boundaries.
"""
